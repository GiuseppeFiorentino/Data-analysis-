{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Homework**\n",
        "\n",
        "Complete the following tasks:\n",
        "\n",
        "* Use a dataset of 21 Video Sessions\n",
        "* Recognize the Video Server(s) IP and select video traffic (***if more than one Server is found, keep the dominant flow only***)\n",
        "* Detect Video Client HTTP Requests (Uplink packets with size larger or equal to 100 Bytes)\n",
        "* Compute features to predict:\n",
        " 1.   When the next UL Request is sent by the Video Client \n",
        " 2.   How large is the response of the Server to the next UL Request\n",
        "\n",
        "**N.B.**: Below, you can find a list of useful functions for the tasks at hand (introduced during class)."
      ],
      "metadata": {
        "id": "EgRb4mQBd2UY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "zJxa-o59dwkB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions Ready-To-Use"
      ],
      "metadata": {
        "id": "7DsJgewH_DVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_traffic(data, domain):\n",
        "   \n",
        "    # Look in DNS Responses for googlevideo domain\n",
        "    dns_data = data[data['Protocol']=='DNS']\n",
        "    dns = dns_data[dns_data['Info'].apply(lambda x: 'googlevideo' in x and 'response' in x)]\n",
        "    ips = dns.Address.values \n",
        "    server_names = dns.Name.values\n",
        "    \n",
        "    # Filtering on either \"Source\" or \"Destination\" IP, get the \n",
        "    # rows of the dataset that contain at least one of the selected IPs\n",
        "    downlink = data[data['Source'].apply(lambda x: x in ips)].dropna(subset=['Length']) \n",
        "\n",
        "    uplink = data[data['Destination'].apply(lambda x: x in ips)].dropna(subset=['Length'])\n",
        "    \n",
        "    return ips, server_names, uplink, downlink\n",
        "\n",
        "def find_dominant(uplink, downlink):\n",
        "\n",
        "  # Expressed in MB\n",
        "\n",
        "  # Order flows by cumulative DL Volume\n",
        "  flows_DL = downlink.groupby(['Source','Destination'])['Length'].sum()/(10**6)\n",
        "  print(flows_DL)\n",
        "  \n",
        "  # Get (Source,Destination) IPs of dominant flows\n",
        "  dom_id = flows_DL[flows_DL==max(flows_DL)].index[0]\n",
        "\n",
        "  # Filter traffic selecting the dominant flow\n",
        "  dom_dl = downlink[downlink['Source']==dom_id[0]]\n",
        "  dom_ul = uplink[(uplink['Source']==dom_id[1])]\n",
        "\n",
        "  return dom_ul, dom_dl\n",
        "\n",
        "def timebased_filter(data, length=None, min_time=None, max_time=None):\n",
        "  '''\n",
        "  :param data: pd dataframe to be filtered. Must contain columns: \"Length\" and \"Time\"\n",
        "  :param length: all packets shorter than length [Bytes] will be discarded (default 0)\n",
        "  :param min_time: all packets with timestamp smaller than min_time [s] will be discarded (default 0)\n",
        "  :param max_time: all packets with timestamp larger than max_time [s] will be discarded (default 1000)\n",
        "  '''\n",
        "\n",
        "  if length is None:\n",
        "    length=0\n",
        "  if min_time is None:\n",
        "    min_time = 0\n",
        "  if max_time is None:\n",
        "    max_time = 1000\n",
        "  \n",
        "  filtered_data = data.copy().reset_index()\n",
        "  mask = (filtered_data['Length']>=length) & (filtered_data['Time']>=min_time) & (filtered_data['Time']<= max_time)\n",
        "  filtered_data = filtered_data.loc[mask[mask ==True].index]\n",
        "\n",
        "  return filtered_data\n",
        "\n",
        "def find_next(array, value):\n",
        "  '''\n",
        "  :param array: np.array, array of floats\n",
        "  :param value: float, reference value\n",
        "  :return: position of the closest element of the array greater than \"value\"\n",
        "  '''\n",
        "  delta = np.asarray(array) - value\n",
        "  idx = np.where(delta >= 0, delta, np.inf).argmin()\n",
        "\n",
        "  return idx\n",
        "\n",
        "def normalize_dataset(training_set, test_set):\n",
        "\n",
        "  mean_train = training_set.mean()\n",
        "  std_train = training_set.std()\n",
        "  norm_train = (training_set - mean_train)/std_train\n",
        "  norm_test = (test_set - mean_train)/std_train  \n",
        "\n",
        "  return norm_train, norm_test"
      ],
      "metadata": {
        "id": "-EKk7Xqfd6gI"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions to be completed\n"
      ],
      "metadata": {
        "id": "jMO_nFab_IPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def features_extraction(uplink, downlink):\n",
        "  '''\n",
        "  Complete this function to extract both features and groundtruth.\n",
        "\n",
        "  NB: The features extraction process is the same as the one introduced during\n",
        "  the lecture. \n",
        "  '''\n",
        "  dataset = pd.DataFrame(columns=['Request_Size','Inter_RR_Time','DL_Time','DL_Vol','DL_Size','PB_Time'])\n",
        "  # ****************************************************************************\n",
        "  # Feature 1: Client Request Size(Size of Last HTTP request)\n",
        "  dataset['Request_Size'] = list(uplink.Length.values)\n",
        "  \n",
        "  # ****************************************************************************\n",
        "  # Feature 2: Inter Request-Response Time\n",
        "  rr_time = []\n",
        "  response_time = []\n",
        "  for t in uplink.Time:\n",
        "    response_time.append(find_next(downlink.Time, t)) #index of next DL packet timestamp \n",
        "    rr_time.append(downlink.Time.iloc[response_time[-1]] - t)\n",
        "\n",
        "  dataset['Inter_RR_Time'] = rr_time\n",
        "\n",
        "  # ****************************************************************************\n",
        "  # Feature 3-4-5: Download Time, Download Volume, Download Size (# Packets) \n",
        "  dt = []\n",
        "  dv = []\n",
        "  ds = []\n",
        "  \n",
        "  # Needed two structures in which volumes and times will be added\n",
        "  next_burst_size_list = []\n",
        "  next_request_time_list = []\n",
        " \n",
        "  for rt1, rt2 in zip(response_time[:-1], response_time[1:]):\n",
        "\n",
        "    #Download Time\n",
        "    dt.append(downlink.Time.iloc[rt2-1] - downlink.Time.iloc[rt1])\n",
        "\n",
        "    temp = timebased_filter(downlink, 0, downlink.Time.iloc[rt1], downlink.Time.iloc[rt2-1])\n",
        "    \n",
        "    #Download Volume\n",
        "    dv.append(temp.Length.sum())\n",
        "\n",
        "    #a list to be used to populate groundtruth dataframe\n",
        "    next_burst_size_list.append(dv[-1])\n",
        "    #I have applied the find next function based on the hint given \n",
        "    next_request_time_list.append(uplink.Time.iloc[find_next(uplink.Time, downlink.Time.iloc[rt2-1])] - downlink.Time.iloc[rt2-1])\n",
        "    \n",
        "    #Download Size (# Packets) \n",
        "    ds.append(temp.shape[0])\n",
        "\n",
        "  # Last Iteration data might be corrupted due to drastic interruption of capture \n",
        "  # process. If it is so, an error would occur during the features extraction.\n",
        "  # To avoid this, we skip last HTTP iteration data when an error is raised \n",
        "  # using the try-except logic below.\n",
        "  try:\n",
        "    # Consider also last HTTP iteration\n",
        "    #Download Time\n",
        "    dt.append(downlink.Time.iloc[-1] - downlink.Time.iloc[rt2])\n",
        "\n",
        "    temp = timebased_filter(downlink, 0, downlink.Time.iloc[rt2], downlink.Time.iloc[-1])\n",
        "    #Download Volume\n",
        "    dv.append(temp.Length.sum())\n",
        "\n",
        "    #Download Size (# Packets) \n",
        "    ds.append(temp.shape[0])\n",
        "  except:\n",
        "    print()\n",
        "\n",
        "  dataset['DL_Time'] = dt\n",
        "  dataset['DL_Vol'] = dv\n",
        "  dataset['DL_Size'] = ds\n",
        "\n",
        "  # ****************************************************************************\n",
        "  # Feature 5: Playback Time\n",
        "  pbt = list(uplink.Time.values)\n",
        "  dataset['PB_Time'] = pbt\n",
        "  # ****************************************************************************\n",
        "  \n",
        "  \n",
        "  # Check Features Consistency\n",
        "  dataset = dataset[(dataset > 0).all(1)]\n",
        "  dataset = dataset[dataset['DL_Time']<20]\n",
        "\n",
        "  ###############################################################\n",
        "  # TO BE COMPLETED\n",
        "\n",
        "  ### EXTRACT GROUNDTRUTH HERE\n",
        "  groundtruth = pd.DataFrame(columns=['Next_Request_Time','Next_Response_Vol'])\n",
        "  # ****************************************************************************\n",
        "  # GT 1: Next Request Time\n",
        "  #The arrival time of the next HTTP Request is the time between the last DL packet and the next “large” UL packet\n",
        "  groundtruth['Next_Request_Time'] = next_request_time_list\n",
        "  # ****************************************************************************\n",
        "  # GT 2: Next Response Volume\n",
        "  #his groundtruth comes from free from \"Server Burst Volume\" feature…\n",
        "  groundtruth['Next_Response_Vol'] = next_burst_size_list\n",
        "\n",
        "  ###############################################################\n",
        "\n",
        "\n",
        "  # Check Ground Truth Consistency\n",
        "  groundtruth.dropna(inplace=True)\n",
        "\n",
        "  # Align Dataset and Groundtruth\n",
        "  intersection = set(dataset.index).intersection(set(groundtruth.index))\n",
        "  dataset = dataset.loc[intersection,:]\n",
        "  groundtruth = groundtruth.loc[intersection,:]\n",
        " \n",
        "\n",
        "  return dataset, groundtruth"
      ],
      "metadata": {
        "id": "Fi212kIMeIXr"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "d45P-M-v_PKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Drive is uploaded in order to select all the capture files."
      ],
      "metadata": {
        "id": "0yJ4axjeeULG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "id": "4prKO9dxf_pB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89e4e929-35d1-458e-e266-c38119ce221a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "QsckjQpefugN"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In these first lines of code, a for loop is used to iterate over all the files contained in the '/content/gdrive/MyDrive/homework3' path.\n",
        "Then only the relevant traffic is considered and after found the dominant up and dowlink traffic the dataframes are populated."
      ],
      "metadata": {
        "id": "VQw93UApeopO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/homework3'\n",
        "tcpdumpfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "gt= np.unique([f.split('.')[0] for f in tcpdumpfiles])\n",
        "\n",
        "uplink = pd.DataFrame()\n",
        "downlink = pd.DataFrame()\n",
        "\n",
        "for f in tcpdumpfiles:\n",
        "  capture = pd.read_csv(join(path,f))\n",
        "  \n",
        "  domain_name = 'googlevideo'\n",
        "\n",
        "  #filter only relevant traffic\n",
        "  ips, server_names, uplink, downlink = filter_traffic(capture,domain_name)\n",
        "\n",
        "  # for each capture consider just the dominant traffic flows\n",
        "  dom_ul, dom_dl = find_dominant(uplink, downlink)\n",
        "  \n",
        "  #applying the features extraction function\n",
        "  dataset, groundtruth = features_extraction(dom_ul, dom_dl)\n",
        "\n",
        "  #print(dataset)"
      ],
      "metadata": {
        "id": "26ibGTtpfnhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921f706c-2b6b-4ad6-e699-63344830793e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source          Destination\n",
            "74.125.163.138  192.168.1.6    0.008266\n",
            "74.125.99.91    192.168.1.6    0.008056\n",
            "Name: Length, dtype: float64\n",
            "Source          Destination\n",
            "74.125.104.103  192.168.1.6     0.012088\n",
            "74.125.111.106  192.168.1.6     0.015628\n",
            "74.125.154.138  192.168.1.6     1.054021\n",
            "91.81.217.140   192.168.1.6    17.478906\n",
            "Name: Length, dtype: float64\n",
            "Source          Destination\n",
            "74.125.111.105  192.168.1.6    7.992543\n",
            "74.125.99.105   192.168.1.6    0.009947\n",
            "91.81.217.140   192.168.1.6    2.965944\n",
            "Name: Length, dtype: float64\n",
            "Source         Destination\n",
            "74.125.153.24  192.168.1.6    4.127591\n",
            "Name: Length, dtype: float64\n",
            "Source          Destination\n",
            "74.125.111.106  192.168.1.6     2.080363\n",
            "91.81.217.141   192.168.1.6    32.572815\n",
            "Name: Length, dtype: float64\n",
            "Source           Destination\n",
            "173.194.160.200  192.168.1.6    0.839776\n",
            "173.194.187.71   192.168.1.6    1.781121\n",
            "74.125.105.10    192.168.1.6    0.012088\n",
            "91.81.217.140    192.168.1.6    8.902818\n",
            "Name: Length, dtype: float64\n",
            "Source           Destination\n",
            "173.194.188.105  192.168.1.6    1.686488\n",
            "Name: Length, dtype: float64\n",
            "Source           Destination\n",
            "173.194.160.219  192.168.1.6    0.008240\n",
            "173.194.188.72   192.168.1.6    1.974872\n",
            "74.125.153.59    192.168.1.6    0.012162\n",
            "74.125.99.108    192.168.1.6    0.015762\n",
            "Name: Length, dtype: float64\n",
            "Source         Destination\n",
            "91.81.217.140  192.168.1.6    2.106974\n",
            "Name: Length, dtype: float64\n",
            "Source         Destination\n",
            "209.85.226.38  192.168.1.6    0.549426\n",
            "74.125.162.39  192.168.1.6    0.015674\n",
            "74.125.162.40  192.168.1.6    2.380683\n",
            "74.125.99.170  192.168.1.6    0.015766\n",
            "Name: Length, dtype: float64\n",
            "Source         Destination\n",
            "91.81.217.141  192.168.1.6    0.359344\n",
            "Name: Length, dtype: float64\n",
            "Source         Destination\n",
            "74.125.99.169  192.168.1.6    0.602666\n",
            "91.81.217.140  192.168.1.6    3.844219\n",
            "91.81.217.141  192.168.1.6    0.012511\n",
            "Name: Length, dtype: float64\n",
            "Source           Destination\n",
            "172.217.132.137  192.168.1.6    0.414788\n",
            "Name: Length, dtype: float64\n",
            "Source           Destination\n",
            "173.194.182.138  192.168.1.6    0.015734\n",
            "173.194.187.136  192.168.1.6    1.495762\n",
            "74.125.110.102   192.168.1.6    0.866399\n",
            "74.125.160.202   192.168.1.6    2.283527\n",
            "74.125.4.230     192.168.1.6    0.005583\n",
            "74.125.99.106    192.168.1.6    1.023713\n",
            "74.125.99.72     192.168.1.6    1.731761\n",
            "91.81.217.140    192.168.1.6    6.745528\n",
            "91.81.217.141    192.168.1.6    0.021178\n",
            "Name: Length, dtype: float64\n",
            "Source           Destination\n",
            "173.194.188.136  192.168.1.6    0.158705\n",
            "74.125.111.102   192.168.1.6    1.924227\n",
            "74.125.153.11    192.168.1.6    0.008252\n",
            "74.125.99.166    192.168.1.6    1.296750\n",
            "74.125.99.168    192.168.1.6    0.652067\n",
            "Name: Length, dtype: float64\n",
            "Source           Destination\n",
            "173.194.182.135  192.168.1.6    0.017631\n",
            "74.125.99.168    192.168.1.6    0.199115\n",
            "Name: Length, dtype: float64\n",
            "Source         Destination\n",
            "74.125.99.137  192.168.1.6    1.441036\n",
            "Name: Length, dtype: float64\n",
            "Source           Destination\n",
            "173.194.182.230  192.168.1.6    1.706543\n",
            "173.194.188.230  192.168.1.6    0.853296\n",
            "74.125.153.7     192.168.1.6    1.060220\n",
            "74.125.99.108    192.168.1.6    1.938626\n",
            "Name: Length, dtype: float64\n",
            "Source         Destination\n",
            "74.125.160.38  192.168.1.6    0.008204\n",
            "74.125.99.168  192.168.1.6    0.410873\n",
            "74.125.99.170  192.168.1.6    2.693258\n",
            "Name: Length, dtype: float64\n",
            "Source         Destination\n",
            "91.81.217.141  192.168.1.6    9.993622\n",
            "Name: Length, dtype: float64\n",
            "Source         Destination\n",
            "91.81.217.140  192.168.1.6    44.867013\n",
            "91.81.217.141  192.168.1.6    13.785031\n",
            "Name: Length, dtype: float64\n",
            "Source         Destination\n",
            "91.81.217.140  192.168.1.6    48.812378\n",
            "Name: Length, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Task\n",
        "\n",
        "In this part of the homework is achieved the regression task on the two continuos quantities 'Next_Request_Time' and 'Next_Response_Vol'. \n",
        "It is used a Random Forest regressor using as criterion the minimum square error.\n",
        "The procedure followed was to split the dataframe into two subsets and on those distinctly is applied the regressor.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "70dVrct8f46m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "random_forest_Regressor = RandomForestRegressor(n_estimators=1,criterion = 'mse')\n",
        "\n",
        "#list used to calculate the minimum mean square error\n",
        "mse_nrt_list = []\n",
        "mse_nrv_list = []\n",
        "\n",
        "#splitting the groundtruth into two dataframes used to find the mean square errors\n",
        "nrt = groundtruth['Next_Request_Time']\n",
        "nrv = groundtruth['Next_Response_Vol']\n"
      ],
      "metadata": {
        "id": "Utg7efg7Ut_9"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and prediction of the next http request**"
      ],
      "metadata": {
        "id": "lXsHf17jnUmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for train_index,test_index in kf.split(dataset):\n",
        "\n",
        "  #defining train and test sets\n",
        "  data_train,data_test = dataset.iloc[train_index,:], dataset.iloc[test_index,:]\n",
        "  labels_train,labels_test = nrt.iloc[train_index], nrt.iloc[test_index]\n",
        "  \n",
        "  #normalizing train and test sets  \n",
        "  norm_train, norm_test = normalize_dataset(data_train,data_test)\n",
        "\n",
        "  #applying regression alg\n",
        "  random_forest_Regressor.fit(norm_train,labels_train)\n",
        "\n",
        "  mse_nrt = random_forest_Regressor.predict(norm_test)\n",
        "\n",
        "  mse_nrt_list.append(metrics.mean_squared_error(labels_test,mse_nrt))\n"
      ],
      "metadata": {
        "id": "3f6HBhConIb7"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RMSE ARRIVAL TIME NEXT HTTP REQUEST"
      ],
      "metadata": {
        "id": "6l4tkGbLkF71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The mse obtained on the arrival time of the next http request is: ')\n",
        "print(\"%.3f\" % math.sqrt(metrics.mean_squared_error(labels_test,mse_nrt)*1000), 'ms')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "822J4lFybvUU",
        "outputId": "0ceba2c5-ea76-44df-816d-f09aeb04d0c8"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mse obtained on the arrival time of the next http request is: \n",
            "0.003 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and prediction of the next burst volume**"
      ],
      "metadata": {
        "id": "xPYUrKkZm6Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for train_index,test_index in kf.split(dataset):\n",
        "\n",
        "  #defining train and test sets\n",
        "  data_train2,data_test2 = dataset.iloc[train_index,:], dataset.iloc[test_index,:]\n",
        "  labels_train2,labels_test2 = nrv.iloc[train_index], nrv.iloc[test_index]\n",
        "  \n",
        "  #normalizing train and test sets  \n",
        "  norm_train2, norm_test2 = normalize_dataset(data_train2,data_test2)\n",
        "  \n",
        "  #applying regression alg\n",
        "  random_forest_Regressor.fit(norm_train2,labels_train2)\n",
        "\n",
        "  mse_nrv = random_forest_Regressor.predict(norm_test2)\n",
        "\n",
        "  mse_nrv_list.append(metrics.mean_squared_error(labels_test2,mse_nrv))\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Qzed98zFYNpV"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RMSE NEXT BURST VOLUME"
      ],
      "metadata": {
        "id": "s2C6bXFkkRmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The mse obtained on the volume for the next burst is: ')\n",
        "print(\"%.3f\" % math.sqrt(metrics.mean_squared_error(labels_test2,mse_nrv)),'Kb')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVCcESN-TxHV",
        "outputId": "f218ed54-40cd-4bf0-b006-affa04a510be"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mse obtained on the volume for the next burst is: \n",
            "328.086 Kb\n"
          ]
        }
      ]
    }
  ]
}